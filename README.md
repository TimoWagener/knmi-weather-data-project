# KNMI Weather Data Lakehouse ğŸŒ¦ï¸

A production-ready data lakehouse for Dutch weather data using **medallion architecture** (Bronze/Silver/Gold layers) with **93.5% API optimization** through multi-station batch loading.

Downloads hourly weather observations from the KNMI EDR API, processes through data quality layers, and provides efficient querying with DuckDB, Polars, and Pandas.

![Data Architecture](https://img.shields.io/badge/Architecture-Medallion-blue)
![Python](https://img.shields.io/badge/Python-3.10%2B-green)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Optimization](https://img.shields.io/badge/API_Efficiency-93.5%25-brightgreen)

## ğŸŒŸ Features

- ğŸš€ **93.5% API Optimization**: Multi-station batch loading (v2) - load 8 stations with 156 API calls vs 2,400!
- âœ… **Medallion Architecture**: Bronze (raw) â†’ Silver (validated) â†’ Gold (aggregated)
- âœ… **Modern Stack**: DuckDB, Polars, Pandas, Parquet, Python 3.10+
- âœ… **Data Quality**: Automated validation, outlier detection, quality scoring
- âœ… **Efficient Storage**: Columnar Parquet with monthly partitioning
- âœ… **Fast Queries**: Sub-second queries on millions of records
- âœ… **77 Stations Available**: Access to all KNMI weather stations across Netherlands
- âœ… **Scalable**: Can load 48-60 stations per session (entire KNMI network in 2-3 hours!)
- âœ… **Metadata Tracking**: Automated load status, gap detection, resume capability
- âœ… **Secure**: API keys in .env, not in code

## ğŸ“Š Current Data

- **Stations Loaded**: 2 complete, 8 loading (10 total configured)
  - âœ… Hupsel (rural, eastern Netherlands)
  - âœ… Deelen (Veluwe, airport)
  - ğŸ”„ De Bilt, Schiphol, Rotterdam, Vlissingen, Maastricht, Eelde, Den Helder, Twenthe (loading now)
- **Coverage**: 316,278 hours (2000-2025, 25+ years per station)
- **Parameters**: 23 weather measurements (temp, humidity, rainfall, wind, pressure, solar, visibility, etc.)
- **Storage**: ~292 MB across all layers (Bronze Raw: 206 MB, Refined: 42 MB, Silver: 44 MB)
- **Quality**: Automated scoring, outlier detection, data validation on all records

## ğŸš€ Quick Start

### 1. Clone & Setup

```bash
git clone <repository-url>
cd LocalWeatherDataProject

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

```bash
# Copy template
cp .env.example .env

# Edit .env and add your KNMI API keys
# Get keys from: https://developer.dataplatform.knmi.nl/
```

### 3. Download Data

```bash
# ğŸš€ v2 Multi-Station Batch Loading (RECOMMENDED - 93.5% fewer API calls!)
# Load all 10 configured stations (2000-2025, ~90-120 minutes)
python src/orchestrate_historical_v2.py --stations core_10 --start-year 2000 --end-year 2025 --batch-size 8 --chunk-months 2

# Or manual single-station pipeline (for custom date ranges)
python src/ingest_bronze_raw.py --station hupsel --start-date "2024-01-01T00:00:00Z" --end-date "2024-12-31T23:59:59Z"
python src/transform_bronze_refined.py --station hupsel --year 2024
python src/transform_silver.py --station hupsel --year 2024
```

### 4. Query the Data

```bash
# Run demo queries
python src/query_demo.py
```

## ğŸ“ Project Structure

```
LocalWeatherDataProject/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ CLAUDE.md             # Guidance for Claude Code AI
â”œâ”€â”€ PROJECT_STATUS.md     # Detailed project status
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example         # API key template
â”œâ”€â”€ .gitignore           # Git ignore rules
â”‚
â”œâ”€â”€ src/                 # Main source code
â”‚   â”œâ”€â”€ config.py                       # Configuration
â”‚   â”œâ”€â”€ ingest_bronze_raw.py            # Download from EDR API (v2 multi-station!)
â”‚   â”œâ”€â”€ orchestrate_historical_v2.py    # ğŸš€ Multi-station batch loader (v2)
â”‚   â”œâ”€â”€ transform_bronze_refined.py     # JSON â†’ Parquet
â”‚   â”œâ”€â”€ transform_silver.py             # Validate & clean
â”‚   â”œâ”€â”€ query_demo.py                   # Demo queries (DuckDB/Polars/Pandas)
â”‚   â””â”€â”€ metadata_manager.py             # Metadata tracking
â”‚
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ v1_single_station/              # Legacy code (pre-optimization)
â”‚
â”œâ”€â”€ metadata/            # Orchestration metadata
â”‚   â”œâ”€â”€ stations_config.json         # 10 station registry
â”‚   â”œâ”€â”€ load_metadata.json           # Load history tracking
â”‚   â””â”€â”€ pipeline_config.json         # Pipeline settings
â”‚
â”œâ”€â”€ logs/                # Orchestration logs
â”‚
â”œâ”€â”€ scripts/             # Utility scripts
â”‚   â”œâ”€â”€ test_edr_api.py
â”‚   â”œâ”€â”€ test_multi_station_api.py    # Multi-station API tester
â”‚   â””â”€â”€ explore_*.py
â”‚
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ API_OPTIMIZATION_OPPORTUNITIES.md  # v2 optimization analysis
â”‚   â”œâ”€â”€ MULTI_STATION_OPTIMIZATION_SUMMARY.md  # Complete v2 guide
â”‚   â”œâ”€â”€ API_RESEARCH_FINDINGS.md
â”‚   â”œâ”€â”€ ARCHITECTURE_PLAN.md
â”‚   â””â”€â”€ EDR_VS_OPEN_DATA_COMPARISON.md
â”‚
â”œâ”€â”€ notebooks/           # Jupyter notebooks (future)
â”œâ”€â”€ tests/              # Unit tests (future)
â””â”€â”€ data/               # Data files (not in git)
    â”œâ”€â”€ bronze/
    â”‚   â”œâ”€â”€ raw/        # Immutable JSON from API
    â”‚   â””â”€â”€ refined/    # Queryable Parquet
    â”œâ”€â”€ silver/         # Validated & cleaned
    â””â”€â”€ gold/           # Aggregated (not yet built)
```

## ğŸ—ï¸ Architecture

### Medallion Layers

**Bronze Raw** (Immutable Source of Truth)
- Format: JSON (exact EDR API responses)
- Purpose: Compliance, reprocessing, debugging
- Size: ~5.7 MB for 2024-2025

**Bronze Refined** (Schema-on-Read)
- Format: Parquet (columnar, compressed)
- Schema: Dynamic (preserves all source fields)
- Size: ~650 KB for 2024-2025

**Silver** (Validated & Cleaned)
- Format: Parquet with fixed schema
- Features: Quality scoring, outlier detection, deduplication
- Size: ~750 KB for 2024-2025

**Gold** (Not Yet Built)
- Purpose: Aggregated metrics, multi-station comparisons
- Future: Daily summaries, station comparisons, dashboards

### Data Flow

```
KNMI EDR API
    â†“ (Monthly chunks)
Bronze Raw (JSON)
    â†“ (Flatten structure)
Bronze Refined (Parquet, schema-on-read)
    â†“ (Validate, clean, score quality)
Silver (Parquet, fixed schema)
    â†“ (Aggregate, model)
Gold (Business-ready)
```

## ğŸ“– Usage Examples

### Multi-Station Batch Loading (v2 - Recommended!)

```bash
# Load multiple stations efficiently (93.5% fewer API calls!)
python src/orchestrate_historical_v2.py \
  --stations core_10 \
  --start-year 2000 \
  --end-year 2025 \
  --batch-size 8 \
  --chunk-months 2

# Check load status
python -c "from src.metadata_manager import MetadataManager; MetadataManager().print_status_summary()"
```

### Single Station (manual pipeline)

```bash
# See available stations
python scripts/test_edr_api.py

# Download single station
python src/ingest_bronze_raw.py --station de_bilt --start-date "2024-01-01T00:00:00Z" --end-date "2024-12-31T23:59:59Z"
python src/transform_bronze_refined.py --station de_bilt --year 2024
python src/transform_silver.py --station de_bilt --year 2024
```

### Query with DuckDB

```python
import duckdb

con = duckdb.connect()

# Query all stations, all years
result = con.execute("""
    SELECT
        YEAR(timestamp) as year,
        AVG(temperature_celsius) as avg_temp,
        SUM(rainfall_mm) as total_rain
    FROM 'data/silver/**/*.parquet'
    GROUP BY year
    ORDER BY year
""").df()

print(result)
```

### Incremental Updates (Multi-Station!)

```bash
# Update ALL 10 stations with yesterday's data (v2 feature - one API call!)
python src/ingest_bronze_raw.py \
  --stations hupsel,deelen,de_bilt,schiphol,rotterdam,vlissingen,maastricht,eelde,den_helder,twenthe \
  --start-date "2025-11-17T00:00:00Z" \
  --end-date "2025-11-17T23:59:59Z"

# Then transform each (can be automated)
for station in hupsel deelen de_bilt schiphol rotterdam vlissingen maastricht eelde den_helder twenthe; do
  python src/transform_bronze_refined.py --station $station --year 2025
  python src/transform_silver.py --station $station --year 2025
done
```

## ğŸš€ v2 Multi-Station Optimization (NEW!)

**Key Achievement: 93.5% Reduction in API Calls**

### What Changed?

**Before (v1 - Single Station):**
- Load 8 stations for 25 years: **2,400 API calls**
- Each station queried separately, each month independently
- Limited scalability (10-15 stations per session max)

**After (v2 - Multi-Station Batching):**
- Load 8 stations for 25 years: **156 API calls** (93.5% fewer!)
- Batch multiple stations in one API call using comma-separated IDs
- Optimal chunk sizing (2-month chunks, 8 stations per batch)
- Can load 48-60 stations per session

### Performance Impact

| Metric | v1 Single-Station | v2 Multi-Station | Improvement |
|--------|-------------------|------------------|-------------|
| API calls (8 stations, 25 years) | 2,400 | 156 | **93.5% fewer** |
| Stations per session (1000 limit) | 10-15 | 48-60 | **4-6x more** |
| Scalability to 70+ stations | Multiple days | 2-3 sessions | **10x faster** |

### Technical Details

```python
# API supports comma-separated station IDs
location_param = ",".join(["station1", "station2", "station3"])
url = f"{EDR_BASE_URL}/collections/{COLLECTION}/locations/{location_param}"

# One call returns data for all stations in CoverageCollection format
# Automatically split and saved per station for backward compatibility
```

**Data point calculation:**
- API limit: 376,000 data points per request
- Our config: 8 stations Ã— 1,440 hours (2 months) Ã— 23 params = 264,960 points (70% of limit)
- Safe, efficient, and maximizes throughput!

**See full analysis:** `docs/MULTI_STATION_OPTIMIZATION_SUMMARY.md`

## ğŸ”§ Configuration

Edit `src/config.py` to customize:

- **Stations**: Add more weather stations
- **Date Ranges**: Define custom time periods
- **Parameters**: Select specific weather variables
- **Paths**: Change data storage locations

## ğŸ“š Documentation

- ğŸš€ **[Multi-Station Optimization Summary](docs/MULTI_STATION_OPTIMIZATION_SUMMARY.md)**: Complete v2 optimization guide
- ğŸš€ **[API Optimization Opportunities](docs/API_OPTIMIZATION_OPPORTUNITIES.md)**: Detailed v2 analysis
- **[Project Status](PROJECT_STATUS.md)**: Current status & comprehensive documentation
- **[CLAUDE.md](CLAUDE.md)**: Guidance for Claude Code AI assistant
- **[API Research](docs/API_RESEARCH_FINDINGS.md)**: KNMI API capabilities & limits
- **[Architecture Plan](docs/ARCHITECTURE_PLAN.md)**: Detailed architecture design
- **[EDR vs Open Data](docs/EDR_VS_OPEN_DATA_COMPARISON.md)**: API comparison

## ğŸ¤ Contributing

This is a personal project, but feel free to fork and adapt for your own use!

## ğŸ“ License

MIT License - feel free to use and modify

## ğŸ™ Acknowledgments

- **KNMI** (Royal Netherlands Meteorological Institute) for providing free weather data
- **EDR API** following OGC Environmental Data Retrieval standards
- Built with **DuckDB**, **Parquet**, and **Python**

## ğŸ“§ Support

For issues or questions:
1. Check [PROJECT_STATUS.md](PROJECT_STATUS.md) for detailed documentation
2. Review [docs/](docs/) for architecture details
3. See KNMI API docs: https://developer.dataplatform.knmi.nl/

---

## ğŸ¯ Current Status

**Last Updated**: 2025-11-17
**Version**: v2 (Multi-Station Optimization)
**Status**:
- âœ… v2 Multi-station optimization complete (93.5% API reduction)
- âœ… Bronze & Silver layers production-ready
- âœ… 2 stations fully loaded (Hupsel, Deelen)
- ğŸ”„ 8 additional stations loading now
- â³ Gold layer pending (future)

**Data**:
- 316,278 hours across 2 stations (2000-2025)
- 10 stations configured and ready
- ~1.75 million hours when current load completes

**Performance**:
- **93.5% fewer API calls** vs v1 single-station approach
- Can load 48-60 stations per session (under 1000 API call limit)
- Entire KNMI network (70+ stations) feasible in 2-3 sessions

**Next Steps**:
1. Complete 8-station load (in progress)
2. Build automated daily updater
3. Expand to 20-30 more stations
4. Create Gold layer for aggregated analytics
